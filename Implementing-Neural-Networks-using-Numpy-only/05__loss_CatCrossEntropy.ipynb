{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27dfac39",
   "metadata": {},
   "source": [
    "## Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d2f4dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7db7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "# An example output from the output layer of the neural network\n",
    "softmax_output = [0.7, 0.1, 0.2]\n",
    "# Ground truth\n",
    "target_output = [1, 0, 0]\n",
    "loss = -(math.log(softmax_output[0])*target_output[0] +\n",
    "         math.log(softmax_output[1])*target_output[1] +\n",
    "         math.log(softmax_output[2])*target_output[2])\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56c9dbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
    "                   [0.1, 0.5, 0.4],\n",
    "                   [0.02, 0.9, 0.08]])\n",
    "class_targets = [0, 1, 1] # dog, cat, cat\n",
    "# print(softmax_outputs[[0, 1, 2], class_targets])\n",
    "\n",
    "print(-np.log(softmax_outputs[\n",
    "      range(len(softmax_outputs)), class_targets]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1eab59",
   "metadata": {},
   "source": [
    "## merging everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc223c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8eed27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "# Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "# Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "# Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5192ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_ReLU:\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "# Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c66542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "# Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,keepdims=True))\n",
    "# Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef292471",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "# Calculates the data and regularization losses\n",
    "# given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "# Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "# Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "# Return loss\n",
    "        return data_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e4dd6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "# Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "# Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "# Clip data to prevent division by 0\n",
    "# Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "# Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true,axis=1)\n",
    "# Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c75cd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use data of spiral data\n",
    "def create_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, samples)\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, samples) + np.random.randn(samples)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8e4437b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333341 0.33333309 0.3333335 ]\n",
      " [0.33333312 0.33333305 0.33333384]\n",
      " [0.33333258 0.33333318 0.33333425]\n",
      " [0.33333242 0.33333304 0.33333454]]\n",
      "loss: 1.0986181849640213\n"
     ]
    }
   ],
   "source": [
    "X, y = create_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 3 input features (as we take output\n",
    "# of previous layer here) and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "# Create Softmax activation (to be used with Dense layer):\n",
    "activation2 = Activation_Softmax()\n",
    "# Create loss function\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "# Perform a forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# it takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "# Let's see output of the first few samples:\n",
    "print(activation2.output[:5])\n",
    "# Perform a forward pass through activation function\n",
    "# it takes the output of second dense layer here and returns loss\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "# Print loss value\n",
    "print('loss:', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb23d93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
