{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21148227",
   "metadata": {},
   "source": [
    "## Stochastic_Gradient_Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1a61c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use data of spiral data\n",
    "def spiral_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, samples)\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, samples) + np.random.randn(samples)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be448bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "427ab227",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "# Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "# Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "# Remember input values\n",
    "        self.inputs = inputs\n",
    "# Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "# Backward pass\n",
    "    def backward(self, dvalues):\n",
    "# Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "# Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f716a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "# Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    def backward(self, dvalues):\n",
    "# Since we need to modify original variable,\n",
    "# let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "# Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f3342d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "# Remember input values\n",
    "        self.inputs = inputs\n",
    "# Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,keepdims=True))\n",
    "# Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "        self.output = probabilities\n",
    "# Backward pass\n",
    "    def backward(self, dvalues):\n",
    "# Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "# Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "# Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "# Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "# Calculate sample-wise gradient\n",
    "# and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a2fdb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "# Calculates the data and regularization losses\n",
    "# given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "# Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "# Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "# Return loss\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fdc43d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "# Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "# Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "# Clip data to prevent division by 0\n",
    "# Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "# Probabilities for target values -\n",
    "# only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true,axis=1)\n",
    "# Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "# Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "# Number of samples\n",
    "        samples = len(dvalues)\n",
    "# Number of labels in every sample\n",
    "# We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "# If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "# Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "# Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52ad7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "# Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "# Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "# Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "# Set the output\n",
    "        self.output = self.activation.output\n",
    "# Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "# Number of samples\n",
    "        samples = len(dvalues)\n",
    "# If labels are one-hot encoded,\n",
    "# turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "# Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "# Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "# Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77efe5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer_SGD:\n",
    "# Initialize optimizer - set settings,\n",
    "# learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "# Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87e57c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.247, loss: 1.099, \n",
      "epoch: 100, acc: 0.410, loss: 1.080, \n",
      "epoch: 200, acc: 0.443, loss: 1.073, \n",
      "epoch: 300, acc: 0.427, loss: 1.072, \n",
      "epoch: 400, acc: 0.433, loss: 1.071, \n",
      "epoch: 500, acc: 0.423, loss: 1.071, \n",
      "epoch: 600, acc: 0.427, loss: 1.070, \n",
      "epoch: 700, acc: 0.423, loss: 1.069, \n",
      "epoch: 800, acc: 0.413, loss: 1.067, \n",
      "epoch: 900, acc: 0.437, loss: 1.065, \n",
      "epoch: 1000, acc: 0.440, loss: 1.062, \n",
      "epoch: 1100, acc: 0.447, loss: 1.058, \n",
      "epoch: 1200, acc: 0.417, loss: 1.060, \n",
      "epoch: 1300, acc: 0.427, loss: 1.054, \n",
      "epoch: 1400, acc: 0.393, loss: 1.047, \n",
      "epoch: 1500, acc: 0.397, loss: 1.051, \n",
      "epoch: 1600, acc: 0.410, loss: 1.036, \n",
      "epoch: 1700, acc: 0.420, loss: 1.036, \n",
      "epoch: 1800, acc: 0.400, loss: 1.042, \n",
      "epoch: 1900, acc: 0.413, loss: 1.041, \n",
      "epoch: 2000, acc: 0.407, loss: 1.039, \n",
      "epoch: 2100, acc: 0.387, loss: 1.038, \n",
      "epoch: 2200, acc: 0.407, loss: 1.042, \n",
      "epoch: 2300, acc: 0.413, loss: 1.026, \n",
      "epoch: 2400, acc: 0.453, loss: 1.025, \n",
      "epoch: 2500, acc: 0.490, loss: 1.019, \n",
      "epoch: 2600, acc: 0.477, loss: 1.004, \n",
      "epoch: 2700, acc: 0.480, loss: 0.979, \n",
      "epoch: 2800, acc: 0.430, loss: 1.032, \n",
      "epoch: 2900, acc: 0.467, loss: 0.974, \n",
      "epoch: 3000, acc: 0.503, loss: 0.967, \n",
      "epoch: 3100, acc: 0.463, loss: 0.972, \n",
      "epoch: 3200, acc: 0.527, loss: 0.944, \n",
      "epoch: 3300, acc: 0.517, loss: 1.001, \n",
      "epoch: 3400, acc: 0.483, loss: 0.952, \n",
      "epoch: 3500, acc: 0.480, loss: 0.964, \n",
      "epoch: 3600, acc: 0.520, loss: 0.936, \n",
      "epoch: 3700, acc: 0.517, loss: 0.912, \n",
      "epoch: 3800, acc: 0.623, loss: 0.896, \n",
      "epoch: 3900, acc: 0.603, loss: 0.852, \n",
      "epoch: 4000, acc: 0.607, loss: 0.876, \n",
      "epoch: 4100, acc: 0.557, loss: 0.855, \n",
      "epoch: 4200, acc: 0.553, loss: 0.880, \n",
      "epoch: 4300, acc: 0.613, loss: 0.827, \n",
      "epoch: 4400, acc: 0.603, loss: 0.816, \n",
      "epoch: 4500, acc: 0.583, loss: 0.835, \n",
      "epoch: 4600, acc: 0.443, loss: 1.123, \n",
      "epoch: 4700, acc: 0.580, loss: 0.856, \n",
      "epoch: 4800, acc: 0.553, loss: 0.921, \n",
      "epoch: 4900, acc: 0.603, loss: 0.830, \n",
      "epoch: 5000, acc: 0.567, loss: 0.892, \n",
      "epoch: 5100, acc: 0.573, loss: 0.853, \n",
      "epoch: 5200, acc: 0.533, loss: 0.935, \n",
      "epoch: 5300, acc: 0.670, loss: 0.786, \n",
      "epoch: 5400, acc: 0.633, loss: 0.802, \n",
      "epoch: 5500, acc: 0.610, loss: 0.804, \n",
      "epoch: 5600, acc: 0.523, loss: 0.980, \n",
      "epoch: 5700, acc: 0.690, loss: 0.744, \n",
      "epoch: 5800, acc: 0.597, loss: 0.761, \n",
      "epoch: 5900, acc: 0.600, loss: 0.794, \n",
      "epoch: 6000, acc: 0.610, loss: 0.785, \n",
      "epoch: 6100, acc: 0.567, loss: 0.864, \n",
      "epoch: 6200, acc: 0.517, loss: 1.001, \n",
      "epoch: 6300, acc: 0.627, loss: 0.743, \n",
      "epoch: 6400, acc: 0.677, loss: 0.718, \n",
      "epoch: 6500, acc: 0.607, loss: 0.815, \n",
      "epoch: 6600, acc: 0.520, loss: 1.156, \n",
      "epoch: 6700, acc: 0.510, loss: 1.044, \n",
      "epoch: 6800, acc: 0.587, loss: 0.808, \n",
      "epoch: 6900, acc: 0.627, loss: 0.720, \n",
      "epoch: 7000, acc: 0.637, loss: 0.715, \n",
      "epoch: 7100, acc: 0.600, loss: 0.750, \n",
      "epoch: 7200, acc: 0.603, loss: 0.764, \n",
      "epoch: 7300, acc: 0.633, loss: 0.788, \n",
      "epoch: 7400, acc: 0.590, loss: 0.772, \n",
      "epoch: 7500, acc: 0.590, loss: 0.756, \n",
      "epoch: 7600, acc: 0.593, loss: 0.766, \n",
      "epoch: 7700, acc: 0.597, loss: 0.754, \n",
      "epoch: 7800, acc: 0.600, loss: 0.730, \n",
      "epoch: 7900, acc: 0.680, loss: 0.700, \n",
      "epoch: 8000, acc: 0.473, loss: 1.215, \n",
      "epoch: 8100, acc: 0.610, loss: 0.776, \n",
      "epoch: 8200, acc: 0.597, loss: 0.750, \n",
      "epoch: 8300, acc: 0.603, loss: 0.737, \n",
      "epoch: 8400, acc: 0.603, loss: 0.778, \n",
      "epoch: 8500, acc: 0.587, loss: 0.808, \n",
      "epoch: 8600, acc: 0.603, loss: 0.717, \n",
      "epoch: 8700, acc: 0.683, loss: 0.681, \n",
      "epoch: 8800, acc: 0.607, loss: 0.752, \n",
      "epoch: 8900, acc: 0.607, loss: 0.742, \n",
      "epoch: 9000, acc: 0.680, loss: 0.689, \n",
      "epoch: 9100, acc: 0.500, loss: 1.117, \n",
      "epoch: 9200, acc: 0.613, loss: 0.720, \n",
      "epoch: 9300, acc: 0.610, loss: 0.754, \n",
      "epoch: 9400, acc: 0.650, loss: 0.786, \n",
      "epoch: 9500, acc: 0.657, loss: 0.732, \n",
      "epoch: 9600, acc: 0.700, loss: 0.679, \n",
      "epoch: 9700, acc: 0.693, loss: 0.667, \n",
      "epoch: 9800, acc: 0.653, loss: 0.737, \n",
      "epoch: 9900, acc: 0.713, loss: 0.657, \n",
      "epoch: 10000, acc: 0.617, loss: 0.783, \n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD()\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "# Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    if not epoch % 100:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ')\n",
    "# Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "# Update weights and biases\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de563bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac732b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
