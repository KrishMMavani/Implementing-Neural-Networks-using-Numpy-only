{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1a61c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use data of spiral data\n",
    "def spiral_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, samples)\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, samples) + np.random.randn(samples)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "be448bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "427ab227",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "# Layer initialization\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "# Initialize weights and biases\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "# Remember input values\n",
    "        self.inputs = inputs\n",
    "# Calculate output values from inputs, weights and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "# Backward pass\n",
    "    def backward(self, dvalues):\n",
    "# Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "# Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f716a69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember input values\n",
    "        self.inputs = inputs\n",
    "# Calculate output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    def backward(self, dvalues):\n",
    "# Since we need to modify original variable,\n",
    "# let's make a copy of values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "# Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5f3342d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "# Forward pass\n",
    "    def forward(self, inputs):\n",
    "# Remember input values\n",
    "        self.inputs = inputs\n",
    "# Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1,keepdims=True))\n",
    "# Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1,keepdims=True)\n",
    "        self.output = probabilities\n",
    "# Backward pass\n",
    "    def backward(self, dvalues):\n",
    "# Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "# Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "# Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "# Calculate Jacobian matrix of the output and\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "# Calculate sample-wise gradient\n",
    "# and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a2fdb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "# Calculates the data and regularization losses\n",
    "# given model output and ground truth values\n",
    "    def calculate(self, output, y):\n",
    "# Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "# Calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "# Return loss\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8fdc43d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "# Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "# Number of samples in a batch\n",
    "        samples = len(y_pred)\n",
    "# Clip data to prevent division by 0\n",
    "# Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "# Probabilities for target values -\n",
    "# only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples),y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true,axis=1)\n",
    "# Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "# Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "# Number of samples\n",
    "        samples = len(dvalues)\n",
    "# Number of labels in every sample\n",
    "# We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "# If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "# Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "# Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52ad7819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
    "# Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "# Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "# Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "# Set the output\n",
    "        self.output = self.activation.output\n",
    "# Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "# Number of samples\n",
    "        samples = len(dvalues)\n",
    "# If labels are one-hot encoded,\n",
    "# turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "# Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "# Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "# Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77efe5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Optimizer_SGD:\n",
    "# # Initialize optimizer - set settings,\n",
    "# # learning rate of 1. is default for this optimizer\n",
    "#     def __init__(self, learning_rate=1.0):\n",
    "#         self.learning_rate = learning_rate\n",
    "# # Update parameters\n",
    "#     def update_params(self, layer):\n",
    "#         layer.weights += -self.learning_rate * layer.dweights\n",
    "#         layer.biases += -self.learning_rate * layer.dbiases\n",
    "\n",
    "# FOR LEARNING RATE DECAY : -\n",
    "class Optimizer_SGD:\n",
    "# Initialize optimizer - set settings,\n",
    "# learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1., decay=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "# Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "# Update parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "# Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87e57c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.390, loss: 1.099, lr: 1\n",
      "epoch: 10, acc: 0.457, loss: 1.098, lr: 0.9910802775024778\n",
      "epoch: 20, acc: 0.453, loss: 1.098, lr: 0.9813542688910698\n",
      "epoch: 30, acc: 0.440, loss: 1.098, lr: 0.9718172983479106\n",
      "epoch: 40, acc: 0.427, loss: 1.097, lr: 0.9624639076034649\n",
      "epoch: 50, acc: 0.423, loss: 1.095, lr: 0.9532888465204957\n",
      "epoch: 60, acc: 0.417, loss: 1.093, lr: 0.9442870632672333\n",
      "epoch: 70, acc: 0.427, loss: 1.089, lr: 0.9354536950420954\n",
      "epoch: 80, acc: 0.433, loss: 1.084, lr: 0.9267840593141798\n",
      "epoch: 90, acc: 0.440, loss: 1.079, lr: 0.9182736455463728\n",
      "epoch: 100, acc: 0.427, loss: 1.075, lr: 0.9099181073703367\n",
      "epoch: 110, acc: 0.420, loss: 1.073, lr: 0.9017132551848512\n",
      "epoch: 120, acc: 0.420, loss: 1.072, lr: 0.8936550491510277\n",
      "epoch: 130, acc: 0.427, loss: 1.071, lr: 0.8857395925597874\n",
      "epoch: 140, acc: 0.430, loss: 1.070, lr: 0.8779631255487269\n",
      "epoch: 150, acc: 0.447, loss: 1.069, lr: 0.8703220191470844\n",
      "epoch: 160, acc: 0.447, loss: 1.068, lr: 0.8628127696289905\n",
      "epoch: 170, acc: 0.447, loss: 1.067, lr: 0.8554319931565441\n",
      "epoch: 180, acc: 0.450, loss: 1.066, lr: 0.8481764206955046\n",
      "epoch: 190, acc: 0.450, loss: 1.066, lr: 0.8410428931875525\n",
      "epoch: 200, acc: 0.453, loss: 1.065, lr: 0.8340283569641367\n",
      "epoch: 210, acc: 0.453, loss: 1.065, lr: 0.8271298593879238\n",
      "epoch: 220, acc: 0.450, loss: 1.064, lr: 0.8203445447087776\n",
      "epoch: 230, acc: 0.450, loss: 1.064, lr: 0.8136696501220504\n",
      "epoch: 240, acc: 0.450, loss: 1.063, lr: 0.8071025020177562\n",
      "epoch: 250, acc: 0.457, loss: 1.063, lr: 0.8006405124099278\n",
      "epoch: 260, acc: 0.460, loss: 1.063, lr: 0.7942811755361399\n",
      "epoch: 270, acc: 0.453, loss: 1.063, lr: 0.7880220646178092\n",
      "epoch: 280, acc: 0.453, loss: 1.062, lr: 0.7818608287724785\n",
      "epoch: 290, acc: 0.453, loss: 1.062, lr: 0.7757951900698216\n",
      "epoch: 300, acc: 0.457, loss: 1.062, lr: 0.7698229407236336\n",
      "epoch: 310, acc: 0.453, loss: 1.062, lr: 0.7639419404125287\n",
      "epoch: 320, acc: 0.457, loss: 1.061, lr: 0.7581501137225171\n",
      "epoch: 330, acc: 0.457, loss: 1.061, lr: 0.7524454477050414\n",
      "epoch: 340, acc: 0.457, loss: 1.061, lr: 0.7468259895444361\n",
      "epoch: 350, acc: 0.457, loss: 1.061, lr: 0.7412898443291327\n",
      "epoch: 360, acc: 0.460, loss: 1.061, lr: 0.7358351729212657\n",
      "epoch: 370, acc: 0.453, loss: 1.060, lr: 0.7304601899196493\n",
      "epoch: 380, acc: 0.450, loss: 1.060, lr: 0.7251631617113851\n",
      "epoch: 390, acc: 0.457, loss: 1.060, lr: 0.7199424046076314\n",
      "epoch: 400, acc: 0.457, loss: 1.060, lr: 0.7147962830593281\n",
      "epoch: 410, acc: 0.453, loss: 1.060, lr: 0.7097232079488999\n",
      "epoch: 420, acc: 0.457, loss: 1.060, lr: 0.704721634954193\n",
      "epoch: 430, acc: 0.450, loss: 1.059, lr: 0.6997900629811057\n",
      "epoch: 440, acc: 0.450, loss: 1.059, lr: 0.6949270326615705\n",
      "epoch: 450, acc: 0.450, loss: 1.059, lr: 0.6901311249137336\n",
      "epoch: 460, acc: 0.450, loss: 1.059, lr: 0.6854009595613434\n",
      "epoch: 470, acc: 0.447, loss: 1.059, lr: 0.6807351940095303\n",
      "epoch: 480, acc: 0.447, loss: 1.059, lr: 0.676132521974307\n",
      "epoch: 490, acc: 0.447, loss: 1.059, lr: 0.671591672263264\n",
      "epoch: 500, acc: 0.447, loss: 1.059, lr: 0.66711140760507\n",
      "epoch: 510, acc: 0.447, loss: 1.059, lr: 0.6626905235255136\n",
      "epoch: 520, acc: 0.440, loss: 1.059, lr: 0.6583278472679394\n",
      "epoch: 530, acc: 0.440, loss: 1.058, lr: 0.6540222367560498\n",
      "epoch: 540, acc: 0.440, loss: 1.058, lr: 0.649772579597141\n",
      "epoch: 550, acc: 0.440, loss: 1.058, lr: 0.6455777921239509\n",
      "epoch: 560, acc: 0.437, loss: 1.058, lr: 0.6414368184733803\n",
      "epoch: 570, acc: 0.437, loss: 1.058, lr: 0.6373486297004461\n",
      "epoch: 580, acc: 0.437, loss: 1.058, lr: 0.6333122229259025\n",
      "epoch: 590, acc: 0.437, loss: 1.058, lr: 0.6293266205160478\n",
      "epoch: 600, acc: 0.437, loss: 1.058, lr: 0.6253908692933083\n",
      "epoch: 610, acc: 0.437, loss: 1.058, lr: 0.6215040397762586\n",
      "epoch: 620, acc: 0.437, loss: 1.057, lr: 0.6176652254478073\n",
      "epoch: 630, acc: 0.437, loss: 1.057, lr: 0.6138735420503376\n",
      "epoch: 640, acc: 0.437, loss: 1.057, lr: 0.6101281269066504\n",
      "epoch: 650, acc: 0.437, loss: 1.057, lr: 0.6064281382656155\n",
      "epoch: 660, acc: 0.437, loss: 1.057, lr: 0.6027727546714888\n",
      "epoch: 670, acc: 0.437, loss: 1.057, lr: 0.5991611743559018\n",
      "epoch: 680, acc: 0.437, loss: 1.057, lr: 0.5955926146515783\n",
      "epoch: 690, acc: 0.437, loss: 1.056, lr: 0.5920663114268798\n",
      "epoch: 700, acc: 0.437, loss: 1.056, lr: 0.5885815185403178\n",
      "epoch: 710, acc: 0.437, loss: 1.056, lr: 0.5851375073142188\n",
      "epoch: 720, acc: 0.437, loss: 1.056, lr: 0.5817335660267597\n",
      "epoch: 730, acc: 0.437, loss: 1.056, lr: 0.578368999421631\n",
      "epoch: 740, acc: 0.437, loss: 1.055, lr: 0.5750431282346177\n",
      "epoch: 750, acc: 0.437, loss: 1.055, lr: 0.5717552887364208\n",
      "epoch: 760, acc: 0.437, loss: 1.055, lr: 0.5685048322910745\n",
      "epoch: 770, acc: 0.437, loss: 1.055, lr: 0.5652911249293385\n",
      "epoch: 780, acc: 0.437, loss: 1.054, lr: 0.5621135469364812\n",
      "epoch: 790, acc: 0.437, loss: 1.054, lr: 0.5589714924538848\n",
      "epoch: 800, acc: 0.437, loss: 1.054, lr: 0.5558643690939411\n",
      "epoch: 810, acc: 0.440, loss: 1.053, lr: 0.5527915975677169\n",
      "epoch: 820, acc: 0.440, loss: 1.053, lr: 0.5497526113249038\n",
      "epoch: 830, acc: 0.440, loss: 1.053, lr: 0.5467468562055767\n",
      "epoch: 840, acc: 0.440, loss: 1.052, lr: 0.5437737901033171\n",
      "epoch: 850, acc: 0.440, loss: 1.052, lr: 0.5408328826392644\n",
      "epoch: 860, acc: 0.440, loss: 1.052, lr: 0.5379236148466918\n",
      "epoch: 870, acc: 0.440, loss: 1.051, lr: 0.5350454788657036\n",
      "epoch: 880, acc: 0.440, loss: 1.051, lr: 0.532197977647685\n",
      "epoch: 890, acc: 0.440, loss: 1.051, lr: 0.5293806246691372\n",
      "epoch: 900, acc: 0.440, loss: 1.050, lr: 0.526592943654555\n",
      "epoch: 910, acc: 0.447, loss: 1.050, lr: 0.5238344683080146\n",
      "epoch: 920, acc: 0.447, loss: 1.050, lr: 0.5211047420531527\n",
      "epoch: 930, acc: 0.450, loss: 1.049, lr: 0.5184033177812338\n",
      "epoch: 940, acc: 0.450, loss: 1.049, lr: 0.5157297576070139\n",
      "epoch: 950, acc: 0.453, loss: 1.048, lr: 0.513083632632119\n",
      "epoch: 960, acc: 0.453, loss: 1.048, lr: 0.5104645227156712\n",
      "epoch: 970, acc: 0.457, loss: 1.048, lr: 0.5078720162519046\n",
      "epoch: 980, acc: 0.457, loss: 1.047, lr: 0.5053057099545225\n",
      "epoch: 990, acc: 0.463, loss: 1.047, lr: 0.5027652086475616\n",
      "epoch: 1000, acc: 0.463, loss: 1.046, lr: 0.5002501250625312\n"
     ]
    }
   ],
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 3 output values (output values)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "# Create Softmax classifier's combined loss and activation\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_SGD(learning_rate=1, decay=1e-3)\n",
    "# Train in loop\n",
    "for epoch in range(1001):\n",
    "# Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "# Perform a forward pass through the activation/loss function\n",
    "# takes the output of second dense layer here and returns loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# calculate values along first axis\n",
    "    predictions = np.argmax(loss_activation.output, axis=1)\n",
    "    if len(y.shape) == 2:\n",
    "        y = np.argmax(y, axis=1)\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    \n",
    "    if not epoch % 10:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate}')\n",
    "# Backward pass\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "# Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac732b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9314af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0963a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c048f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
